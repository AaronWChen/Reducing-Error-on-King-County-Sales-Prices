{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Submission\n",
    "\n",
    "Please fill out:\n",
    "* Student name: Dirk Van Curan\n",
    "* Student pace: Full time\n",
    "* Scheduled project review date/time:  August 23, 2019 9:30 am\n",
    "* Instructor name: Cristian\n",
    "* Blog post URL:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - remember to use markdown cells for comments as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This file contains some in progress code on our Mod 1 project.\n",
    "In this project, we are given real estate sales data from King\n",
    "County in Seattle, and we are asked to make a predictor for price.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules needed for data analysis and get them ready for use in the notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the given data and store it in a pandas DataFrame\n",
    "raw_data = pd.read_csv(\"kc_house_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial exploration of what the data looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decided to use only the zipcodes of each sale as a initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the zipcodes to an array of One Hot values instead of a column of integers\n",
    "# Zipcodes are not really useful as integers since they are actually categorical \n",
    "# values and being in a zipcode or not is more helpful for the model\n",
    "ohe = OneHotEncoder(drop='first', categories='auto') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_zip_trans = ohe.fit_transform(raw_data['zipcode'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes the One Hot encoded array and makes a new sparse DataFrame\n",
    "# the column names are the zipcodes and the values are Boolean 1s or 0s\n",
    "# depending on if the sale happened in that zipcode\n",
    "zip_sparce = pd.DataFrame(price_zip_trans.todense(), columns=ohe.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the sale prices as a new column to the sparse array so that it can be\n",
    "# used in a model\n",
    "zip_sparce['price'] = raw_data['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial examination of correlation between price and zipcode\n",
    "zip_sparce.corr()['price'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_array = np.asarray(zip_sparce['price'])\n",
    "\n",
    "price_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(24,3)})\n",
    "sns.heatmap(zip_sparce.corr().loc[['price']],vmax=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin doing linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose our predictors for the model\n",
    "X = zip_sparce.drop('price', axis=1)\n",
    "y = zip_sparce['price']\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial rough model for exploration\n",
    "predictors = sm.add_constant(X)\n",
    "model = sm.OLS(y, predictors).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try with improved modeling (not StatsModels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(zip_sparce.drop('price', axis=1), \n",
    "                                                    zip_sparce['price'],\n",
    "                                                   random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a StandardScaler object to scale our data for us.\n",
    "ss = StandardScaler()\n",
    "\n",
    "# Now we'll apply it to our data by using the .fit() and transform() methods.\n",
    "ss.fit(X_train)\n",
    "X_train_sc = ss.transform(X_train)\n",
    "X_test_sc = ss.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can fit a LinearRegression object to our training data!\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(X_test_sc, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the .coef_ attribute to recover the results\n",
    "# of the regression.\n",
    "\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And .intercept_\n",
    "\n",
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erin suggested avoiding groupby to make a bunch of different dataframes. Instead, consider making \"hybrid\" columns that play with the data you are given to give another metric for the model to use.\n",
    "\n",
    "Remember to clean the data as you go!\n",
    "\n",
    "Think about the assumptions of the model you're using and if you need to transform your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = raw_data.groupby('zipcode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in test:\n",
    "    print(name)\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.get_group(98178)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['view'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['waterfront'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = raw_data.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(raw_data['bedrooms'],range=(1,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to do with the columns that have null values?\n",
    "waterfront: A: there are only 146 records on the waterfront. I think we can drop this column\n",
    "view: A: drop this as I don't think this helps. If we convert the nulls to zero, it doesn't seem to affect the value of the column much. Before cleaning, there are only 5,036 records that have view out of ~21,600\n",
    "yr_renovated: A: I think we can convert year renovated to the yr_built value if it is null and then make a new column called age_since_upgrade\n",
    "\n",
    "Erin: What if you looked at sqftliving/ rooms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_nan_dropped = raw_data.dropna(axis='rows', subset=['view', 'price'])\n",
    "view_nan_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_view_and_price = view_nan_dropped.filter(['view', 'price'], axis=1)\n",
    "only_view_and_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_view_and_price.corr()['price'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a surprisingly high correlation. We were expecting no correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(only_view_and_price['view']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_4_views = raw_data.loc[raw_data['view'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(only_4_views['price']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_4_views['price'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think the view data should be dropped as using it would break one of the assumptions of linear regression. Linear regression assumes that all the independent variables follow a normal distribution, but View is so heavily skewed right(?) that it should not be considered a normal distribution. Almost 20k of the 21k records have 0 views.\n",
    "\n",
    "\n",
    "Dirk: These are actually categorical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['waterfront'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(raw_data['waterfront']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar logic should be followed for Waterfront. There are only 146 records with a waterfront with ~3000 nan values. Even if the NaN values were replaced with 1, this would result in a bimodel distribution, not a normal one.\n",
    "\n",
    "Not enough values with a weak correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are looking at prices, the unique IDs should not be relevant for our model. That colummn should also be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_nan_dropped = raw_data.dropna(axis='rows', subset=['waterfront', 'price'])\n",
    "only_wf_and_price = wf_nan_dropped.filter(['waterfront', 'price'], axis=1)\n",
    "only_wf_and_price.corr()['price'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_replace_na_with_0 = raw_data.fillna(0)\n",
    "only_wf_no_nan_and_price = wf_replace_na_with_0.filter(['waterfront', 'price'], axis=1)\n",
    "only_wf_no_nan_and_price.corr()['price'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_replace_na_with_0['waterfront'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yr_built_price = raw_data.filter(['yr_built', 'price'])\n",
    "yr_built_price.corr()['price'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now()\n",
    "type(current_time.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yr_built_price['age_int'] =  current_time.year - raw_data['yr_built']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yr_built_price = yr_built_price.drop('age_datetime',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yr_built_price.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yr_built_price.corr()['price'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation between price and age is weak and thus we should drop age from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yrreno_replace_na_with_0 = raw_data.dropna(0)\n",
    "only_reno_no_nan_and_price = yrreno_replace_na_with_0.filter(['yr_renovated', 'price'], axis=1)\n",
    "only_reno_no_nan_and_price.corr()['price'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also a weak correlation, so we should drop this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_data = raw_data.drop(['id', 'waterfront', 'date', 'yr_built', \n",
    "                              'yr_renovated', 'sqft_living15', 'sqft_lot15'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_data.corr()['price'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_data = working_data.drop('sqft per room', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_data['lat_by_long'] = working_data['lat'] / working_data['long']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we did a one hot encode on view? It would hopefully reduce the importance of view rated 0 but significantly improve the impact of a view rated as 4\n",
    "    The one hot encode on view reduced the correlation between a 4 view and price. If we keep view, we should not do a OHE\n",
    "\n",
    "Should we one hot encode condition?\n",
    "    Condition's correlation is very weak and did not improve by doing a one hot encode. We think we should drop condition as a parameter for the model\n",
    "\n",
    "Can we hybridize view and grade? Or view and condition?\n",
    "\n",
    "Figure out normalize for categories\n",
    "    could do sqft_living, sqft_lot, bedrooms, bathrooms, floors \n",
    "    \n",
    "Is there a way to hybridize sqft_living and sqft_lot?\n",
    "    Add the values?\n",
    "    Division? (living/floors)/lot or lot/living?\n",
    "    Could also do lot - living/floor?\n",
    "    \n",
    "Can we use latitude and longitude? Would we scale to the most expensive lat/long location?\n",
    "\n",
    "Whenever we do more basic correlation exploration, we should say why we're not doing a more detailed analysis\n",
    "    Simple correlation to determine whether or not we should do a more in depth analysis.\n",
    "        \n",
    "    If the correlation is already weak, it isn't likely to help us later\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_price_data = working_data['view'].fillna(0)\n",
    "view_price_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_view_trans = ohe.fit_transform(view_price_data.values.reshape(-1,1))\n",
    "view_sparce = pd.DataFrame(price_view_trans.todense(), columns=ohe.get_feature_names())\n",
    "view_sparce['price'] = working_data['price']\n",
    "view_sparce.corr()['price'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_price_data = working_data['condition']\n",
    "price_cond_trans = ohe.fit_transform(cond_price_data.values.reshape(-1,1))\n",
    "cond_sparce = pd.DataFrame(price_cond_trans.todense(), columns=ohe.get_feature_names())\n",
    "cond_sparce['price'] = working_data['price']\n",
    "cond_sparce.corr()['price'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(working_data['grade'], range=(0,4));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_price_data = working_data['grade']\n",
    "price_grade_trans = ohe.fit_transform(grade_price_data.values.reshape(-1,1))\n",
    "grade_sparce = pd.DataFrame(price_grade_trans.todense(), columns=ohe.get_feature_names())\n",
    "grade_sparce['price'] = working_data['price']\n",
    "grade_sparce.corr()['price'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_grade = working_data['grade'].mean()\n",
    "working_data['grade'] - mean_grade\n",
    "median_grade = working_data['grade'].median()\n",
    "median_grade\n",
    "(working_data['grade'] - median_grade)/median_grade\n",
    "std_grade = working_data['grade'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_grade_hybridize = working_data['view'].fillna(0) * (working_data['grade'] - median_grade)/std_grade\n",
    "#view_grade_hybridize['price'] = working_data['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(working_data['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_grade_dict = {'view_grade_hybrid': view_grade_hybridize, 'price': working_data['price']}\n",
    "view_grade_df = pd.DataFrame(view_grade_dict)\n",
    "view_grade_df.corr()['price'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop 33 bedroom houses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(working_data['price']));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long = working_data.filter(['lat', 'long'])\n",
    "lat_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_data = working_data.drop(['view', 'lat', 'long', 'condition'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minus_33_rooms = working_data[working_data['bedrooms'] != 33] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minus_33_rooms.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = minus_33_rooms[(working_data['bedrooms'] >= 8)]\n",
    "finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['floors'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(raw_data['floors']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_data = working_data.drop(['sqft_above'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_data = working_data.drop('lat_by_long', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minus_33_rooms = working_data[working_data['bedrooms'] != 33] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minus_33_rooms.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Have cleaned data, switching drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
